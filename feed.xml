<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jina0218.github.io/AI810/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jina0218.github.io/AI810/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-02T11:03:49+09:00</updated><id>https://jina0218.github.io/AI810/feed.xml</id><title type="html">AI810 Blog Post (20220175)</title><subtitle>Home to Jina Kim&apos;s Blog Post </subtitle><entry><title type="html">Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations</title><link href="https://jina0218.github.io/AI810/blog/paper-1/" rel="alternate" type="text/html" title="Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations"/><published>2025-06-01T00:00:00+09:00</published><updated>2025-06-01T00:00:00+09:00</updated><id>https://jina0218.github.io/AI810/blog/paper-1</id><content type="html" xml:base="https://jina0218.github.io/AI810/blog/paper-1/"><![CDATA[<h2 id="introduction-why-graph-generation-matters">Introduction: Why Graph Generation Matters</h2> <p>Graphs are fundamental to representing structured relationships in various domains, such as molecular structures in drug discovery<d-cite key="simonovsky2018graphvaegenerationsmallgraphs"></d-cite> and understanding social networks<d-cite key="grover2019graphiteiterativegenerativemodeling"></d-cite>. Unlike data modalities such as images and sequences, graphs include both node-level semantics and edge-level topology, requiring models to simultaneously capture attribute dependencies and structural constraints. Effective graph generation requires not only realism in local features but also global coherence, validity, and invariance to node permutations<d-cite key="you2018graphrnngeneratingrealisticgraphs"></d-cite>.</p> <p>While traditional generative models, such as VAEs, GANs, and autoregressive models, have achieved limited success when applied to graphs due to these complexities, recent advancements in score-based generative modeling<d-cite key="song2021scorebasedgenerativemodelingstochastic"></d-cite> have shown promise in the continuous domain especially for image and molecule synthesis, raising the following question: <strong>can score-based methods be extended to graphs in a way that respects their structural complexities?</strong> This is the key motivation behind this work, <strong>GDSS</strong> (Graph Diffusion via the System of Stochastic differential equations).</p> <h2 id="limitations-of-existing-methods">Limitations of Existing Methods</h2> <p>Autoregressive models like GraphRNN<d-cite key="you2018graphrnngeneratingrealisticgraphs"></d-cite> generate graphs by adding nodes and edges sequentially. While flexible, they are sensitive to node ordering, inefficient for large graphs, and struggle with long-range dependencies.</p> <p>VAE-based models such as GraphVAE<d-cite key="simonovsky2018graphvaegenerationsmallgraphs"></d-cite> and JT-VAE<d-cite key="jin2019junctiontreevariationalautoencoder"></d-cite> rely on encoding entire graphs into continuous latent spaces and decoding them back. However, they often struggle to generate valid graphs and require complex heuristics to enforce constraints.</p> <p>GAN-based models like MolGAN<d-cite key="decao2022molganimplicitgenerativemodel"></d-cite> generate graph structures using adversarial training, which tends to be unstable and prone to mode collapse, especially in high-dimensional or sparse graph settings.</p> <p>Flow and early diffusion models like GraphAF<d-cite key="shi2020graphafflowbasedautoregressivemodel"></d-cite> and GeoDiff<d-cite key="xu2022geodiffgeometricdiffusionmodel"></d-cite> improve sample diversity and allow tractable density estimation. Yet, they often model node features and edge structures independently, neglecting their mutual dependencies.</p> <p>EDP-GNN<d-cite key="niu2020permutationinvariantgraphgeneration"></d-cite> is a score-based model but generates graphs via discrete perturbations of heuristically chosen noise scales to estimate the score function and edge-wise scores and only produce adjacency matrices, overlooking crucial node-edge dependencies in real-world graphs like molecules.</p> <p>While graph generation aims to synthesize graphs that closely match the distribution of observed data, these limitations highlight the need for a model that can jointly model nodes and edges on continuous-time domain, respect graph symmetries, and scale with complexity: and <strong>GDSS</strong> meets this need.</p> <h2 id="how-gdss-works">How GDSS Works</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig0-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/fig0.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Figure 1: Concept figure of GDSS.</a> </div> <p><strong>GDSS</strong> tackles these limitations by proposing a (1) <strong>scored-based graph generation</strong> framework (2) on a <strong>continuous-time</strong> domain that can (3) generate <strong>both</strong> the node features and the adjacency matrix.</p> <p>During training, the model learns <strong>two score functions</strong> that approximate the gradients of the log-density of the noisy data with respect to node features and edges. A <strong>permutation-equivariant score based model</strong> learns these scores, capturing correlations between structure and semantics. This design allows GDSS to simulate the <strong>reverse-time SDEs</strong> and sample realistic graphs from noise, achieving state-of-the-art performance on both synthetic and molecular graph benchmarks.</p> <p>An overview of GDSS’s process is shown below.</p> <h3 id="0-data-representation">0. Data Representation</h3> <p>To start with, GDSS operates on undirected graphs with node features, represented as</p> <p>$G$ = ($X$, $A$): \(X \in \mathbb{R}^{N \times F}, \quad A \in \mathbb{R}^{N \times N}\),</p> <p>where $X$ is the node feature matrix for $N$ nodes, each with dimension $F$ features and $A$ is the symmetric adjacency matrix.</p> <h3 id="1-forward-diffusion-via-coupled-sdes">1. Forward Diffusion via Coupled SDEs</h3> <p>To model the dependency between $X$ and $A$, GDSS formulates a forward diffusion process of graphs that transforms both of them to a simple noise distribution. The forward process \({G_t = (X_t,A_t)}, t∈[0,T]\) is defined by two independent SDEs, as shown below:</p> \[d\mathbf{G}_t = \mathbf{f}_t(\mathbf{G}_t) \, dt + \mathbf{g}_t(\mathbf{G}_t) \, d\mathbf{w}, \quad \mathbf{G}_0 \sim p_{\text{data}}\] <p>where $\mathbf{f}_t$ is the linear draft coefficient, $\mathbf{g}_t$ is the diffusion coefficient and $w$ is standard Wiener processes. $\mathbf{f}_t$ and $\mathbf{g}_t$ are chosen such that at the terminal time horizon $T$, the diffused sample $G_T$ approximately follows a prior distribution that has a tractable form to efficiently generate the samples, such as Gaussian distribution. To make it simpler, \(\mathbf{g}_t(\mathbf{G}_t)\) is chosen to be a scalar function $g_t$.</p> <h3 id="2-reverse-process-with-learned-score-functions">2. Reverse Process with Learned Score Functions</h3> <p>To generate graphs that follow the data distribution, GDSS start from samples of the prior distribution and leverage the following g reverse-time SDE<d-cite key="anderson1982reverse"></d-cite><d-cite key="song2021scorebasedgenerativemodelingstochastic"></d-cite>:</p> \[dG_t = \left[ f_t(G_t) - g_t^2 \nabla_{G_t} \log p_t(G_t) \right] dt + g_t d\bar{\omega},\] <p>where $p_t$ is the marginal distribution under the forward diffusion process at time \(t\), \(\bar{\mathbf{w}}\) is a reverse-time standard Wiener process, and \(d\bar{t}\) is an infinitesimal negative time step.</p> <p>For efficient computing of \(\nabla_{G_t} \log p_t(G_t)\), GDSS utilizes below equations, which operate the same function as above equation. This is a novel approach that interprets the diffusion of a graph as the diffusion of each component that are interrelated through time.</p> \[dX_t = [f_{1, t}(X_t)-g_{1, t}^2 \nabla_{X_t} \log p_t(X_t, A_t)] d\bar{t} + g_{1, t} d\bar{\mathbf{w}}_1\] \[dA_t = [f_{2, t}(A_t)-g_{2, t}^2 \nabla_{A_t} \log p_t(X_t, A_t)] d\bar{t} + g_{2, t} d\bar{\mathbf{w}}_2\] <p>The key property of GDSS is that the diffusion processes in the system are dependent on each other, as shown in the partial score functions, \(\nabla_{X_t} \log p_t(X_t, A_t)\) and \(\nabla_{A_t} \log p_t(X_t, A_t)\). To show the importance of modeling the dependency, Figure 2 compares the generated samples with the data distribution as bivaraite Gaussian mixture, which shows that GDSS successfully handles correlation of two variables while others ((c) GDSS-seq : that generates \(X\) and \(A\) sequentially and not simultaneously or (d) that ignores the diffusion process of \(X\)) can’t.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/toy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/toy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/toy-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/toy.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Figure 2: A toy experiment on modeling the dependency. GDSS successfully models the correlation unlike others.</a> </div> <p>Now, since the exact score functions $\nabla_{X_t} \log p_t$ and $\nabla_{A_t} \log p_t$ are unknown, GDSS learns parameterized approximations as follow:</p> <p>\(s_{\theta, t}(G_t) \approx \nabla_{X_t} \log p_t(G_t), \quad s_A(\phi, t) \approx \nabla_{A_t} \log p_t(G_t)\).</p> <p>Considering that the score-based models should be trained to minimize the distance to the corresponding ground-truth partial scores, GDSS utilizes a novel objectives that generalize the score matching<d-cite key="score1"></d-cite> to the estimation of partial scores for the given graph dataset:</p> \[\min_{\theta} \, \mathbb{E}_t \left\{ \lambda_1(t) \, \mathbb{E}_{G_0} \, \mathbb{E}_{G_t|G_0} \left\| s_{\theta,t}(G_t) - \nabla_{X_t} \log p_t(G_t) \right\|_2^2 \right\}\] \[\min_{\phi} \, \mathbb{E}_t \left\{ \lambda_2(t) \, \mathbb{E}_{G_0} \, \mathbb{E}_{G_t|G_0} \left\| s_{\phi,t}(G_t) - \nabla_{A_t} \log p_t(G_t) \right\|_2^2 \right\}\] <p>However, in above equation, the ground-truth partial scores are not accessible, so GDSS denoises score matching to the partial scores, as shown below:</p> \[\min_{\theta} \, \mathbb{E}_t \left\{ \lambda_1(t) \, \mathbb{E}_{G_0} \, \mathbb{E}_{G_t|G_0} \left\| s_{\theta,t}(G_t) - \nabla_{X_t} \log p_{0t}(G_t | G_0) \right\|_2^2 \right\}\] \[\min_{\phi} \, \mathbb{E}_t \left\{ \lambda_2(t) \, \mathbb{E}_{G_0} \, \mathbb{E}_{G_t|G_0} \left\| s_{\phi,t}(G_t) - \nabla_{A_t} \log p_{0t}(G_t | G_0) \right\|_2^2 \right\}\] <p>Furthermore, since the drift coefficient of the forward diffusion process is linear as shown below,</p> \[d\mathbf{G}_t = \mathbf{f}_t(\mathbf{G}_t) \, dt + \mathbf{g}_t(\mathbf{G}_t) \, d\mathbf{w}, \quad \mathbf{G}_0 \sim p_{\text{data}}\] <p>we can get</p> \[p_{0t}(G_t \mid G_0) = p_{0t}(X_t \mid X_0) \cdot p_{0t}(A_t \mid A_0)\] <p>As a result, the final equation can be summarized as below, with more details for the derivations found in GDSS<d-cite key="gdss"></d-cite> (Appendix A.1, A.2):</p> \[\min_{\theta} \, \mathbb{E}_t \left\{ \lambda_1(t) \, \mathbb{E}_{G_0} \, \mathbb{E}_{G_t|G_0} \left\| s_{\theta,t}(G_t) - \nabla_{X_t} \log p_{0t}(X_t | X_0) \right\|_2^2 \right\}\] \[\min_{\phi} \, \mathbb{E}_t \left\{ \lambda_2(t) \, \mathbb{E}_{G_0} \, \mathbb{E}_{G_t|G_0} \left\| s_{\phi,t}(G_t) - \nabla_{A_t} \log p_{0t}(A_t | A_0) \right\|_2^2 \right\}\] <p>Now, it is important to note that estimating the partial scores of GDSS is different from estimating marginal scores \(\nabla_{X_t} \log p_t(X_t) \quad \text{or} \quad \nabla_{A_t} \log p_t(A_t)\) used in previous score-based generative models because GDSS requires capturing the dependency between node features $X_t$ and adjacency matrices $A_t$ through their joint probability over time. Since above training objectives enable effective estimation of these partial scores, the next step is to design model architectures capable of learning them.</p> <h3 id="3-permuation-equivariant-score-based-model">3. Permuation-equivariant Score-based Model</h3> <p>For this, GDSS propose two neural architectures based on Graph Neural Networks (GNNs).</p> <p>For the adjacency score, the model \(s_{\phi,t}(G_t)\) estimates the gradient \(\nabla_{A_t} \log p_t(X_t, A_t).\) This is done by graph multi-head (GMH) attention, which helps identify key relational patterns between nodes, along with higher-order adjacency matrices to account for long-range interactions. These representations are then passed through a multi-layer perceptron (MLP) to produce the final score.</p> <p>For the node feature score, the model \(s_{\theta,t}(G_t)\) targets \(\nabla_{X_t} \log p_t(X_t, A_t).\) It stacks multiple GNN layers to compute contextual node embeddings from the adjacency matrix, which are again aggregated via an MLP to form the score estimate.</p> <p>Importantly, both models incorporate time conditioning by scaling their outputs with the standard deviation of the diffusion process at time \(t,\) following previous work<d-cite key="song2020generativemodelingestimatinggradients"></d-cite>. Because GNNs and GMH layers are permutation-equivariant, the resulting score functions preserve the permutation invariance of graph distributions, which is an essential property for meaningful graph generation.</p> <h3 id="4-solving-the-reverse-time-sde-system-with-s4">4. Solving the Reverse Time SDE System with S4</h3> <p>Once GDSS learns the partial scores for nodes and edges, the next challenge is generating graphs by solving the <strong>reverse-time diffusion process</strong>:</p> \[\begin{cases} \mathrm{d}\mathbf{X}_t = f_{1,t}(\mathbf{X}_t)\,\mathrm{d}t + g_{1,t}\,\mathrm{d}\bar{\mathbf{w}}_1 \underbrace{-\, g_{1,t}^2\, s_{\theta,t}(\mathbf{X}_t, \mathbf{A}_t)\,\mathrm{d}t}_{\textbf{S}} \\ \mathrm{d}\mathbf{A}_t = f_{2,t}(\mathbf{A}_t)\,\mathrm{d}t + g_{2,t}\,\mathrm{d}\bar{\mathbf{w}}_2 \underbrace{-\, g_{2,t}^2\, s_{\phi,t}(\mathbf{X}_t, \mathbf{A}_t)\,\mathrm{d}t}_{\textbf{S}} \end{cases} \quad \underbrace{\text{First two terms}}_{\textbf{F}}\] <p>These coupled SDEs are not trivial to solve due to the tight interdependence between $X_t$ and $A_t$. To address this, GDSS introduces <strong>S4 (Symmetric Splitting for Systems of SDEs)</strong>, a new solver that balances accuracy and efficiency by leveraging operator splitting techniques. The method is inspired by symmetric splitting samplers<d-cite key="dockhorn2022scorebasedgenerativemodelingcriticallydamped"></d-cite> and the predictor-corrector sampler for SDEs<d-cite key="song2021scorebasedgenerativemodelingstochastic"></d-cite>.</p> <p>Each reverse-time step is decomposed into three phases:</p> <ol> <li><strong>Score Computation:</strong> Estimate partial scores \(s_{\theta,t}\) and \(s_{\phi,t}\) using the trained networks.</li> <li><strong>Correction:</strong> Use a <strong>Langevin MCMC</strong><d-cite key="parisi1981correlation"></d-cite> step to refine the current state \(G_t\), ensuring alignment with the learned score.</li> <li><strong>Prediction:</strong> Use the Fokker-Planck operators for both drift (F) and score (S) terms in above equation to evolve the graph state backward. This is done via the <strong>Trotter splitting</strong><d-cite key="trotter1959product"></d-cite> <d-cite key="strang1968construction"></d-cite> scheme:</li> </ol> \[e^{\delta t L^*_F/2} \, e^{\delta t L^*_S} \, e^{\delta t L^*_F/2}\] <p>where the F-terms correspond to the dynamics of the forward diffusion process and can be sampled exactly from the known transition distribution. However, the S-term which represents score-based updates, is not analytically tractable and is approximated using a simple <strong>Euler method</strong>. This symmetric arrangement ensures second-order accuracy and reduces numerical artifacts that can accumulate during simulation. Further details regarding the derivations can be found in <d-cite key="gdss"></d-cite> (Appendix A.4 and A.5).</p> <p>Compared to traditional predictor-corrector samplers, S4 cuts the number of score network evaluations by half, achieving a significant reduction in compute without sacrificing fidelity. This design choice is critical because score network evaluation is typically the dominant computational bottleneck.</p> <p>Moreover, S4 generalizes to systems involving different types of SDEs, including <strong>Variance Exploding (VE)</strong> and <strong>Variance Preserving (VP)</strong> forms <d-cite key="song2021scorebasedgenerativemodelingstochastic"></d-cite>. This level of generality contrasts with earlier samplers like SSCS, which are restricted to specific SDE structures <d-cite key="dockhorn2022scorebasedgenerativemodelingcriticallydamped"></d-cite>.</p> <p>To further increase accuracy (at higher computational cost), it is possible to use higher-order integrators like the <strong>Runge–Kutta method</strong> or substitute <strong>Hamiltonian Monte Carlo (HMC)</strong> <d-cite key="neal2011mcmc"></d-cite> in place of Langevin dynamics during the correction phase.</p> <h2 id="experimental-results">Experimental Results</h2> <h3 id="generic-graph-generation">Generic Graph Generation</h3> <p>To evaluate the ability of GDSS to generate graphs that match real-world and synthetic graph distributions, a series of experiments on four generic datasets are conducted. These datasets include a mix of small-scale, synthetic, and protein-structured graphs with diverse topologies and statistical properties:</p> <ul> <li><strong>Ego-small</strong>: 200 ego-centric graphs extracted from the Citeseer citation network <d-cite key="sen2008collective"></d-cite>.</li> <li><strong>Community-small</strong>: 100 synthetic graphs constructed from stochastic block models, each representing community structures.</li> <li><strong>Enzymes</strong>: 587 protein tertiary structure graphs from the BRENDA database <d-cite key="schomburg2004brenda"></d-cite>.</li> <li><strong>Grid</strong>: 100 standard 2D grid graphs, known to be challenging due to their regular, structured nature.</li> </ul> <p>The goal is to assess how well GDSS can learn the distribution of these datasets and generate samples with similar structural properties. Following standard graph generation benchmarks <d-cite key="you2018graphrnn"></d-cite>, the evaluation compares distributions of three graph statistics: (1) Node degree (2) Clustering coefficient (3) 4-node orbits.</p> <p>The similarity between generated and real distributions is quantified using <strong>maximum mean discrepancy (MMD)</strong>, computed with a <strong>Gaussian Earth Mover’s Distance (EMD) kernel</strong>, which is more stable and well-defined than total variation distance used in some earlier work <d-cite key="liao2019efficient"></d-cite>. The experiments use the same training/test splits and metrics to ensure a fair comparison.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table1-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/table1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Table 1: Generation results on the generic graph datasets.</a> </div> <p>In Table 1, GDSS demonstrates significant improvements over prior one-shot graph generative models, including the previous score-based method EDP-GNN. GDSS not only surpasses all one-shot baselines but also outperforms most autoregressive models, which are typically more expressive but require much higher computational cost.</p> <p>In particular, on the <strong>Grid dataset</strong>, GDSS shows comparable the performance to GraphRNN, a strong autoregressive baseline, despite being a one-shot model. By contrast, EDP-GNN completely fails to model such structured graphs, highlighting the limitations of adjacency-only generation and discrete transition dynamics.</p> <p>Now, one concern in estimating the joint partial score \(\nabla_{A_t} \log p_t(X_t, A_t)\) is that it may appear significantly harder than estimating simpler, marginal-like scores such as \(\nabla_{A_t} \log p_t(X_0, A_t),\) which ignore the time-evolving dependency between node features and edges. Surprisingly, the empirical results indicate the opposite, as shown in Figure 3.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig1-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/fig1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Figure 3: Complexity of the score-based models.</a> </div> <p>To investigate this, the <strong>complexity of score-based models</strong> is measured using the <strong>squared Frobenius norm of the Jacobians</strong> of their score functions, denoted \(J_F(t)\). This metric reflects the sensitivity of the model and the smoothness of the learned score landscape. GDSS is compared against GDSS-seq, a variant that simplifies score estimation by decoupling node and edge interactions over time.</p> <p>As shown in Figure 3, GDSS consistently exhibits <strong>lower model complexity</strong> than GDSS-seq when trained on the Ego-small dataset. This holds for both partial score functions:</p> \[\nabla_{A_t} \log p_t(X_t, A_t) \quad \text{and} \quad \nabla_{X_t} \log p_t(X_t, A_t)\] <p>This counterintuitive result highlights a key strength of GDSS that, learning the <strong>true joint dynamics between node features and edges</strong> not only improves generative performance (as shown in Table 1), but also simplifies the optimization landscape. The reduced Jacobian norms suggest that joint modeling provides smoother, more learnable gradients, enabling more stable and efficient training.</p> <h3 id="molecule-generation-performance-and-efficiency">Molecule Generation Performance and Efficiency</h3> <p>To evaluate molecular graph generation, the experiments are conducted on two widely used datasets:</p> <ul> <li><strong>QM9</strong> <d-cite key="ramakrishnan2014quantum"></d-cite>: A dataset of small organic molecules with up to 9 heavy atoms.</li> <li><strong>ZINC250k</strong> <d-cite key="irwin2012zinc"></d-cite>: A subset of the ZINC database containing drug-like molecules.</li> </ul> <p>Following prior works <d-cite key="shi2020graphaf"></d-cite><d-cite key="luo2021graphdf"></d-cite>, molecules are kekulized using the RDKit toolkit <d-cite key="landrum2016rdkit"></d-cite>, and hydrogen atoms are removed to simplify structure while preserving core bonding patterns.</p> <p>The evaluation focuses on three aspects:</p> <ul> <li><strong>Fréchet ChemNet Distance (FCD)</strong> <d-cite key="preuer2018frechet"></d-cite>, which computes the distance between generated and real molecules in chemical feature space.</li> <li><strong>NSPDK MMD</strong> <d-cite key="costa2010fast"></d-cite>, a graph-structured MMD metric that captures both node and edge features.</li> <li><strong>Validity w/o correction</strong>, defined as the fraction of chemically valid molecules generated <strong>without any post-processing</strong> or valency correction. Notably, this differs from earlier works by allowing atoms with formal charges, following <d-cite key="zang2020moflow"></d-cite>, to better reflect the diversity of real-world molecular graphs.</li> </ul> <p>All models generate <strong>10,000 molecules</strong>, and generation time is measured in terms of RDKit-formatted outputs. Additional implementation details including hyperparameters are described in Appendix C.3 of the paper.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table2-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/table2.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Table 2: Generation results on the QM9 and ZINC250k dataset.</a> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/fig3-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/fig3.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Figure 4: Visualization of the generated molecules with maximum Tanimoto similarity.</a> </div> <p>As shown in Table 2, GDSS achieves the <strong>highest validity</strong> among all methods <strong>without relying on post-hoc valency correction</strong>. This indicates that GDSS effectively learns the <strong>chemical valency rules</strong>, which fundamentally rely on modeling the relationship between atoms (nodes) and bonds (edges).</p> <p>GDSS also significantly outperforms all baselines in <strong>NSPDK MMD</strong>, and most in <strong>Fréchet ChemNet Distance (FCD)</strong>, demonstrating that the generated molecules are not only structurally realistic in graph space but also chemically plausible in feature space. This shows GDSS’s ability to model complex joint distributions of molecules with diverse node and edge types.</p> <p>Visualizations in Figure 4 further emphasize this result: molecules generated by GDSS often share <strong>large substructures</strong> with those from the training set. In contrast, baseline methods frequently produce molecules with less resemblance or completely unrelated structures, highlighting GDSS’s superior generative fidelity.</p> <p>Beyond quality, GDSS is also highly efficient, as shown in Table 2. On the <strong>QM9 dataset</strong>, GDSS achieves a <strong>450× speed-up</strong> in generation time compared to GraphDF, one of the strongest autoregressive baselines. This efficiency stems from GDSS’s continuous-time diffusion framework, which replaces costly sequential sampling with a more scalable reverse-time SDE simulation.</p> <p>Additionally, both GDSS and its variant GDSS-seq outperform EDP-GNN in generation speed. This further validates that <strong>modeling graph transformation as a continuous diffusion process</strong> is not only more expressive but also much more practical than discrete-step perturbation methods.</p> <h2 id="ablation-studies">Ablation Studies</h2> <h3 id="why-modeling-node-edge-dependency-matters">Why Modeling Node-Edge Dependency Matters</h3> <p>To assess the importance of capturing the <strong>dependency between node features and edges</strong>, GDSS is compared with GDSS-seq, where GDSS models the <strong>joint dependency</strong> between $X$ and $A$, while GDSS-seq assumes that $A$ depends only on $X$, simplifying the reverse SDE system. This subtle design choice turns out to be crucial.</p> <p>Across all benchmarks in <strong>Table 1 (generic graphs)</strong> and <strong>Table 2 (molecules)</strong>, GDSS consistently outperforms GDSS-seq in every metric. The results clearly demonstrate that <strong>accurately modeling the graph distribution requires learning both node and edge interactions through time</strong>, rather than treating them as partially independent.</p> <p>For molecule generation in particular, the benefit is <strong>measurable in terms of chemical validity</strong>. GDSS achieves higher validity scores without needing valency correction, directly showing that it better captures the structure–function relationship between atoms and bonds.</p> <p>These findings confirm that the proposed system of SDEs—where the partial scores</p> \[\nabla_{X_t} \log p_t(X_t, A_t)\] <p>and</p> \[\nabla_{A_t} \log p_t(X_t, A_t)\] <p>are co-trained—provides a more faithful and powerful approach to generative graph modeling.</p> <h3 id="significance-of-s4-solver">Significance of S4 Solver</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-1/table3-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-1/table3.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2202.02514" target="_blank">Table 3: Comparison between fixed step size SDE solvers.</a> </div> <p>As shown in Table 3, <strong>S4 significantly outperforms both Euler–Maruyama (EM) and Reverse sampler</strong>, which are simple baseline solvers for reverse-time diffusion, validating the advantage of including both prediction and correction steps. More impressively, despite using <strong>half the number of score network evaluations</strong>, <strong>S4 also outperforms the PC samplers</strong>, a more advanced methods using Langevin MCMC for correction.</p> <p>This efficiency stems from S4’s use of <strong>symmetric splitting</strong> and <strong>score reuse</strong>, which allow it to maintain high sampling accuracy while dramatically reducing computational overhead. Since score-based model evaluations dominate the cost of solving stochastic differential equations (SDEs), this reduction is critical for scaling to large or complex graphs. By combining <strong>accuracy, efficiency, and generality</strong>, S4 becomes a powerful and practical solver for simulating the reverse diffusion process in GDSS.</p> <h2 id="personal-thoughts">Personal Thoughts</h2> <p><strong>GDSS</strong> redefines the paradigm of graph generation by enabling the <strong>joint</strong>, <strong>continuous</strong>, and <strong>permutation-invariant</strong> synthesis of both <strong>graph structure</strong> and <strong>semantic node features</strong>. It uses the <strong>stochastic differential equations (SDEs)</strong> and the <strong>score-based diffusion modeling</strong> to navigate the complex space of structured data. By coupling <strong>feature-edge diffusion dynamics</strong> and employing <strong>equivariant GNN-based score estimators</strong>, GDSS establishes a framework for generating <strong>valid</strong>, <strong>diverse</strong>, and <strong>semantically coherent</strong> graphs, contributing to areas including, <strong>molecular modeling</strong>, <strong>synthetic biology</strong>, and <strong>network science</strong>.</p> <h3 id="key-takeaways">Key Takeaways</h3> <ul> <li> <p><strong>Unified Joint Modeling</strong><br/> By treating the adjacency matrix and node features as co-evolving under a <strong>system of SDEs</strong>, this work overcomes the typical fragmentation in graph generation. It significantly improves <strong>structural-semantic alignment</strong>, which is crucial for tasks like <strong>molecular graph synthesis</strong>.</p> </li> <li> <p><strong>Permutation-Invariant Score Learning</strong><br/> Leveraging <strong>permutation-equivariant score-based model</strong> ensures that the model respects the graph’s inherent symmetries, meaning that the <strong>node permutations do not affect the outcome</strong>, addressing a key limitation in many autoregressive approaches.</p> </li> <li> <p><strong>Continuous and Flexible Framework</strong><br/> Diffusion-based generative modeling offers <strong>continuous-time flexibility</strong> and avoids common drawbacks like <strong>mode collapse</strong> seen in GANs and VAEs. It is especially suited for <strong>high-dimensional, sparse, and structured</strong> graph data.</p> </li> <li> <p><strong>Empirical Strength</strong><br/> The <strong>GDSS</strong> model achieves <strong>state-of-the-art results</strong> on graph generation tasks, outperforming both <strong>one-shot</strong> and <strong>autoregressive</strong> baselines on benchmarks like <strong>QM9</strong> and <strong>ZINC250k</strong> in terms of <strong>Validity (no correction)</strong>, <strong>NSPDK MMD</strong> (structural fidelity), <strong>FCD</strong> (chemical distribution closeness). Among the SDE types, <strong>VE-SDE</strong> paired with the efficient <strong>S4 solver</strong> delivers the best stability, quality, and <strong>sampling speed</strong>, outperforming existing solvers like EM and PC.</p> </li> </ul> <h3 id="future-research-directions">Future Research Directions</h3> <p>Based on the proposed method, I believe that below approaches could be a possible further research direction.</p> <ul> <li> <p><strong>Conditional Graph Generation</strong><br/> Extending the framework to support <strong>property or topology conditioned generation</strong> would broaden its utility in <strong>drug discovery</strong>, <strong>materials design</strong>, and <strong>graph-based reasoning</strong>.</p> </li> <li> <p><strong>Learning with Hard Constraints</strong><br/> Incorporating <strong>domain-specific constraints</strong>, like <strong>chemical valency rules</strong>, as projection steps or within constraint-aware score functions could significantly improve the validity of generated graphs.</p> </li> <li> <p><strong>Dynamic and Temporal Graphs</strong><br/> Extending to <strong>dynamic graph generation</strong>, where both node features and edges evolve over time, would benefit applications such as <strong>interaction networks</strong>, <strong>financial systems</strong>, and <strong>physical simulations</strong></p> </li> <li> <p><strong>Latent Diffusion for Graphs</strong><br/> Considering latent diffusion models such as Stable Diffusion<d-cite key="rombach2022highresolutionimagesynthesislatent"></d-cite>, I think that exploring <strong>generating graphs in a learned latent space</strong> could be another exiting research approach to improve efficiency without sacrificing fidelity.</p> </li> <li> <p><strong>Multimodal Graph Generation</strong><br/> Graphs often co-occur with other modalities in scientific or social domain, in forms such as text and images. Designing joint diffusion frameworks for <strong>cross-modal graph generation</strong> could be another possible research direction.</p> </li> </ul> <h2 id="extending-gdss-latent-diffusion-for-graphs">Extending GDSS: Latent Diffusion for Graphs</h2> <p>In this section, I outline a potential extension of GDSS, inspired by recent advances in latent diffusion models such as Stable Diffusion <d-cite key="rombach2022highresolutionimagesynthesislatent"></d-cite>.</p> <p>One promising direction for scaling GDSS is to adapt its reverse-time SDE framework to operate in a <strong>latent space</strong>, rather than directly on full-resolution graphs. This could be a valid approach since the latent formulation has been effective in other domains for reducing computational cost while preserving high generative fidelity.</p> <p>We can design the latent diffusion framework for graphs with the following components:</p> <ol> <li> <p><strong>Graph Encoder</strong><br/> A permutation-invariant encoder, such as a GNN with hierarchical pooling, maps each graph \(G = (X, A)\) to a compact latent vector \(z \in \mathbb{R}^d,\) capturing both topological and semantic information.</p> </li> <li> <p><strong>Latent Diffusion Process</strong><br/> A continuous-time SDE is applied in the latent space. The model learns a score function \(s_{\psi,t}(z)\) to reverse the noising process and generate new latent vectors by simulating reverse-time dynamics.</p> </li> <li> <p><strong>Graph Decoder</strong><br/> A decoder reconstructs graph structures from latent representations, possibly using a graph-based VAE, graph transformer, or autoregressive generator conditioned on \(z.\)</p> </li> </ol> <p>This latent-GDSS architecture offers several advantages:</p> <ul> <li><strong>Efficiency</strong>: Operating in the latent space reduces the dimensionality of the generative modeling problem, lowering training and sampling cost.</li> <li><strong>Regularization</strong>: The encoder-decoder structure encourages a more structured, semantically meaningful manifold, improving generalization.</li> <li><strong>Modality Compatibility</strong>: A shared latent space can enable multimodal extensions, such as text-to-graph generation or property-conditioned synthesis.</li> </ul> <p>However, a core challenge is ensuring that the latent space remains <strong>sufficiently smooth and expressive</strong> to support stable diffusion modeling. This could be addressed through contrastive pretraining, joint training with score-based regularization, or denoising objectives that align with the graph decoder.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[Score-based generative models have achieved strong results in image and molecular generation, but applying them to graph-structured data remains challenging due to the complex interplay between node features and graph topology. In this blog, we explore GDSS (Graph Diffusion via the System of Stochastic differential equations), a model that tackles this problem by jointly modeling the evolution of node attributes and adjacency matrices through a system of coupled stochastic differential equations. Using a permutation-equivariant graph neural network and score matching, GDSS generates graphs that maintain both structural validity and semantic coherence.]]></summary></entry><entry><title type="html">Exploring Chemical Space with Score-based Out-of-distribution Generation</title><link href="https://jina0218.github.io/AI810/blog/paper-2/" rel="alternate" type="text/html" title="Exploring Chemical Space with Score-based Out-of-distribution Generation"/><published>2025-06-01T00:00:00+09:00</published><updated>2025-06-01T00:00:00+09:00</updated><id>https://jina0218.github.io/AI810/blog/paper-2</id><content type="html" xml:base="https://jina0218.github.io/AI810/blog/paper-2/"><![CDATA[<h2 id="introduction-the-challenge-of-novel-molecule-generation">Introduction: The Challenge of Novel Molecule Generation</h2> <p>In de novo drug discovery, deep generative models have emerged as powerful tools for automating the design of novel molecules. However, the generated molecules tend to closely resemble those in the training distribution, restricting their usefulness in discovering truly novel compounds with superior therapeutic potential. This is especially problematic when aiming to avoid patented scaffolds or explore uncharted regions of the chemical space. Moreover, real-world drug design often requires satisfying multiple complex property constraints, such as high binding affinity, drug-likeness, and synthesizability. Most existing models optimize simplistic proxy scores, which often result in trivial or unrealistic structures. This blog introduces <strong>MOOD</strong> (Molecular Out-Of-distribution Diffusion), a novel score-based generative framework that addresses these limitations by enabling controlled exploration beyond the training data, while optimizing for multiple drug-relevant properties.</p> <h2 id="limitations-of-existing-models">Limitations of Existing Models</h2> <p>Prior works for molecular generation, such as VAE <d-cite key="G_mez_Bombarelli_2018"></d-cite>, GANs <d-cite key="decao2022molganimplicitgenerativemodel"></d-cite>, and reinforcement learning-based models <d-cite key="olivecrona2017molecularnovodesigndeep"></d-cite>, primarily rely on learning distributions from existing molecular datasets. As a result, they exhibit a strong inductive bias toward the training distribution. For example, the molecules generated by GENTRL<d-cite key="zhavoronkov2019deep"></d-cite> frequently exhibit strong resemblance to known active compounds. Some works attempt to explore novelty through fragment-based RL or prioritized replay<d-cite key="yang2021hit"></d-cite>, but these methods are still constrained by the fragments derived from known molecules and incur high computational costs. Even recent diffusion models like GDSS<d-cite key="jo2022score"></d-cite>, while capable of generating high-quality molecular graphs, lack the mechanism to explicitly control the deviation from training data. Furthermore, these models often optimize over simplified properties (e.g., penalized logP, QED) that do not necessarily correlate with real-world drug efficacy<d-cite key="luo2021graphdf"></d-cite>.</p> <h2 id="mood-a-new-paradigm-for-out-of-distribution-generation">MOOD: A New Paradigm for Out-of-Distribution Generation</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig1-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/fig1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Figure 1: Concept Figure of MOOD.</a> </div> <p>MOOD introduces a new framework for molecule generation that targets two fundamental challenges: (1) generating molecules that are <strong>truly novel and out-of-distribution (OOD)</strong>, and (2) ensuring that these molecules <strong>satisfy real-world drug-like properties</strong>. MOOD achieves this by a novel <strong>OOD-controlled score-based diffusion process</strong>, together with <strong>gradient-based conditional generation</strong>. Unlike traditional reinforcement learning approaches or fragment-based generators, MOOD operates without additional training overhead or sampling complexity, and enables flexible control over how far the generation deviates from the training distribution.</p> <h3 id="score-based-generative-modeling-with-sdes">Score-based Generative Modeling with SDEs</h3> <p>MOOD builds upon GDSS <d-cite key="jo2022score"></d-cite>, which models graphs using a system of stochastic differential equations (SDEs). The molecular graph is represented as \(G_t = (X_t, A_t)\), where \(X_t\) are node features and \(A_t\) is the adjacency matrix.</p> <p>The forward SDE is:</p> \[dG_t = f_t(G_t)\,dt + g_t\,d\omega,\] <p>and the reverse-time diffusion becomes:</p> \[\begin{aligned} dX_t &amp;= \left[ f_{1,t}(X_t) - g_{1,t}^2 \nabla_{X_t} \log p_t(X_t, A_t) \right] dt + g_{1,t} d\bar{\omega}_1, \\\\ dA_t &amp;= \left[ f_{2,t}(A_t) - g_{2,t}^2 \nabla_{A_t} \log p_t(X_t, A_t) \right] dt + g_{2,t} d\bar{\omega}_2, \end{aligned}\] <p>using learned score networks \(s_{\theta_1,t}, s_{\theta_2,t}\). (More details about GDSS can be found in Blog Post “Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations”).</p> <h3 id="expanding-the-exploration-space-with-ood-control">Expanding the Exploration Space with OOD Control</h3> <p>To expand the exploration space of the diffusion, MOOD introduces a novel OOD-controlled score-based graph generative model that can generate samples <strong>outside the in-distribution</strong>, where the OOD-ness is controlled by a hyperparameter \(\lambda \in [0, 1)\).</p> <p>We model this by sampling from the conditional distribution:</p> \[p_t(G_t \mid y_o = \lambda),\] <p>where \(y_o\) denotes the OOD control condition. The reverse-time SDE becomes:</p> \[dG_t = \left[ f_t(G_t) - g_t^2 \nabla_{G_t} \log p_t(G_t \mid y_o = \lambda) \right] dt + g_t d\bar{\omega}. \tag{3}\] <p>The conditional score can be decomposed as:</p> \[\nabla_{G_t} \log p_t(G_t \mid y_o = \lambda) = \nabla_{G_t} \log p_t(G_t) + \nabla_{G_t} \log p_t(y_o = \lambda \mid G_t). \tag{4}\] <p>While \(\nabla_{G_t} \log p_t(G_t)\) can be estimated by the score networks \(s_{\theta_1,t}, s_{\theta_2,t}\), the OOD condition term is modeled as:</p> \[p_t(y_o = \lambda \mid G_t) \propto p_t(G_t)^{-\sqrt{\lambda}}.\] <p>This implies that low-likelihood samples are more likely to be considered OOD, which is inspired by <d-cite key="du2019implicit"></d-cite> and <d-cite key="grathwohl2019your"></d-cite>, encouraging the model to explore underrepresented regions of chemical space.</p> <h3 id="conditional-generation-for-property-optimization">Conditional Generation for Property Optimization</h3> <p>MOOD further modifies generation to favor molecules with desirable chemical properties using conditional generation. Here, the objective is to sample from the joint conditional distribution:</p> \[p_t(G_t \mid y_o = \lambda, y_p),\] <p>where $y_p$ represents a property condition such as high binding affinity, drug-likeness. This is decomposed using Bayes’ rule:</p> \[p_t(G_t \mid y_o = \lambda, y_p) \propto p_t(G_t) \, p_t(y_o = \lambda \mid G_t) \, p_t(y_p \mid G_t, y_o = \lambda).\] <p>The property term \(p_t(y_p \mid G_t, y_o = \lambda)\) is modeled with a Boltzmann distribution:</p> \[p_t(y_p \mid G_t, y_o = \lambda) = \frac{1}{Z_t} \exp\left( \alpha_t P_\phi(G_t, \lambda) \right),\] <p>where $P_\phi$ is a learned property prediction function and $\alpha_t$ is a scaling parameter. Substituting this into the reverse-time SDE gives:</p> \[dG_t = \left[ f_t(G_t) - (1 - \sqrt{\lambda}) g_t^2 \nabla_{G_t} \log p_t(G_t) - \alpha_t g_t^2 \nabla_{G_t} P_\phi(G_t, \lambda) \right] dt + g_t d\bar{\omega},\] <p>where the last term encourages sampling toward regions with higher predicted property values.</p> <p>Therefore, following the form of GDSS<d-cite key="jo2022score"></d-cite>, this can be seperated into node features $X_t$ and adjacency matrix $A_t$ as below:</p> \[\begin{aligned} dX_t &amp;= \left[ f_{1,t}(X_t) - (1 - \sqrt{\lambda}) g_{1,t}^2 s_{\theta_1,t}(X_t, A_t) - \alpha_{1,t} g_{1,t}^2 \nabla_{X_t} P_\phi(X_t, A_t, \lambda) \right] dt + g_{1,t} d\bar{\omega}_1, \\ dA_t &amp;= \left[ f_{2,t}(A_t) - (1 - \sqrt{\lambda}) g_{2,t}^2 s_{\theta_2,t}(X_t, A_t) - \alpha_{2,t} g_{2,t}^2 \nabla_{A_t} P_\phi(X_t, A_t, \lambda) \right] dt + g_{2,t} d\bar{\omega}_2, \end{aligned}\] <p>where $s_{\theta_1,t}$ and $s_{\theta_2,t}$ are score networks approximating the partial derivatives of the log data density with respect to $X_t$ and $A_t$, respectively.</p> <p>To balance the influence of the score and property gradients, MOOD dynamically adjusts the weighting coefficients:</p> \[\alpha_{1,t} = \frac{r_{1,t} \| s_{\theta_1,t}(G_t) \|}{\| \nabla_{X_t} P_\phi(G_t, \lambda) \|}, \quad \alpha_{2,t} = \frac{r_{2,t} \| s_{\theta_2,t}(G_t) \|}{\| \nabla_{A_t} P_\phi(G_t, \lambda) \|},\] <p>where $r_{1,t}$ and $r_{2,t}$ are manually defined scaling ratios. This ensures that the property optimization does not overpower or vanish relative to the diffusion guidance.</p> <h3 id="property-prediction-network-architecture">Property Prediction Network Architecture</h3> <p>To approximate the property function \(P_\phi\), MOOD trains a dedicated neural network on molecule-property pairs. This model predicts properties such as docking score, QED, and synthetic accessibility from graph inputs:</p> \[P_\phi(G_t) := \text{MLP}_s(\tanh(H')),\] <p>where</p> \[H' = \text{MLP}_s\left( \left[ H_0, H_1, ..., H_L \right] \right) \odot \text{MLP}_t\left( \left[ H_0, H_1, ..., H_L \right] \right).\] <p>Here, \(H_0 = X_t\), and each \(H_{l+1} = \text{GNN}(H_l, A_t)\), using a Graph Convolutional Network (GCN). The final feature is passed through two MLPs, one with a sigmoid and the other with a tanh activation, and their outputs are combined by element-wise multiplication to yield the final property score.</p> <p>Overall, by focusing on OOD generation and property optimization, MOOD enables both controlled exploration and the generation of chemically meaningful molecules.</p> <h2 id="experimental-results">Experimental Results</h2> <h3 id="novel-molecule-generation">Novel Molecule Generation</h3> <p>To evaluate MOOD’s ability to generate truly novel molecules, expriment was conducted in unconstrained OOD generation task, so generating molecules without explicitly optimizing for chemical properties. The goal is to validate whether MOOD’s $\lambda$-controlled diffusion process can produce molecules that systematically deviate from the training data distribution. Trained on ZINC250k dataset, MOOD generates 3000 molecules using different values of $\lambda$, with the property optimized term $P_\phi$ removed. The following metrics are used to evaluate the novelty and diversity of the generated molecules:</p> <ul> <li> <p><strong>Fréchet ChemNet Distance (FCD)</strong>: Measures distributional shift in learned chemical representations between training and generated sets<d-cite key="preuer2018frechetchemnetdistancemetric"></d-cite>.</p> </li> <li> <p><strong>NSPDK MMD</strong>: Measures structural differences based on graph kernel statistics<d-cite key="costa2010fast"></d-cite>.</p> </li> <li> <p><strong>Novelty Score</strong>: The fraction of valid molecules that have a Tanimoto similarity less than 0.4 to their closest neighbor in the training data<d-cite key="jin2020multi"></d-cite><d-cite key="xie2021mars"></d-cite>.</p> \[\text{Novelty} = \frac{\text{# of valid molecules with } \text{Tanimoto}(m, m') &lt; 0.4}{\text{Total # of valid generated molecules}}\] </li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig3-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/fig3.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Figure 3: (Left) UMAP visualization of the ZINC250k dataset and the generated molecules by the proposed OOD-controlled diffusion process. (Right) Evaluation results of the molecules generated by the OOD-controlled diffusion.</a> </div> <p>As shown in Figure 3 (a)-(d), increasing λ causes the samples to diverge farther from the training data in the latent space. This visually confirms that MOOD’s OOD-controlled diffusion process offers smooth and tunable control over novelty.</p> <p>Moreover, in the right side of Figure 3, both FCD and NSPDK MMD increase monotonically with λ, suggesting greater divergence in biochemical features and molecular graph structures. The novelty also increases, meaning the generated molecules are not only different in distribution but also chemically unique at the molecular level.</p> <p>Therefore, these results demonstrate that MOOD can generate molecules in a controlled and data-driven manner that generalizes beyond the training distribution—opening doors to the exploration of novel chemical space.</p> <h3 id="property-optimization">Property Optimization</h3> <p>To evaluate whether MOOD can discover compounds that are not only out-of-distribution (OOD) but also biochemically optimal, such as high binding affinity, strong drug-likeness, and good synthetic accessibility, experiment regarding practical demands of real-world de novo drug discovery has been conducted.</p> <p>To reflect multi-objective optimization, the following commposite property function has been defined:</p> \[P_{\text{obj}}(G_t) = \text{DS'}(G_t) \times \text{QED}(G_t) \times \text{SA'}(G_t)\] <p>where</p> <ul> <li><strong>DS’</strong> is the normalized docking score (lower is better)</li> <li><strong>QED</strong> quantifies drug-likeness</li> <li><strong>SA’</strong> measures normalized synthetic accessibility (lower means easier synthesis)</li> </ul> <p>A neural property predictor $P_\phi$ is trained on molecules from the ZINC250k dataset to learn $P_{\text{obj}}$. To evaluate performance, joint measures of novelty and property optimization have been conducted as follows:</p> <ul> <li> <p><strong>Novel Hit Ratio (%)</strong>:<br/> Fraction of unique hit molecules (with DS &lt; known active median, QED &gt; 0.5, SA &lt; 5) that are structurally novel (Tanimoto similarity &lt; 0.4 with training data).</p> </li> <li> <p><strong>Novel Top-5% DS</strong>:<br/> Average docking score of the top 5% unique and novel molecules, ensuring property quality alongside novelty.</p> </li> </ul> <p>To test the generality, the evaluations are performed across five protein targets, <code class="language-plaintext highlighter-rouge">parp1</code>, <code class="language-plaintext highlighter-rouge">fa7</code>, <code class="language-plaintext highlighter-rouge">5ht1b</code>, <code class="language-plaintext highlighter-rouge">braf</code>, and <code class="language-plaintext highlighter-rouge">jak2</code>, with OOD control strength is fixed at $\lambda = 0.04$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table1-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/table1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Table 1: Novel hit ratio (%) results.</a> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table2-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/table2.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Table 2: Novel top 5% docking score (kcal/mol) results.</a> </div> <p>As shown in <strong>Tables 1, 2</strong>, <strong>MOOD achieves state-of-the-art performance</strong> across almost all protein targets.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table3-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/table3.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Table 3: Novel hit ratio (%) results with the similarity condition of 0.3.</a> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table4-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/table4.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Table 4: Novel top 5% docking score (kcal/mol) results with the similarity condition of 0.3.</a> </div> <p>Moreover, it outperforms all baselines in <strong>novel hit ratio</strong> and <strong>top-5% docking scores</strong>, especially under stricter novelty thresholds (Tables 3, 4).</p> <p>Especially, MOOD consistently beats <strong>MOOD-w/o OOD control</strong>, which proves that $\lambda$-based exploration improves discovery. But <strong>MOOD-w/o property predictor</strong> still outperforms GDSS, showing that the OOD mechanism alone adds significant value.</p> <p>Further results in the Appendix (Table 9-13) of the paper<d-cite key="mood"></d-cite> shows that MOOD also maintains <strong>high uniqueness</strong>, <strong>broad structural diversity</strong>, and <strong>robust hit rates</strong>.</p> <p>These findings suggest that MOOD’s <strong>balanced guidance</strong> allows it to find <strong>chemically diverse</strong> and <strong>viable compounds</strong> that elude conventional methods.</p> <h3 id="explorability-visualization">Explorability Visualization</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig4-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/fig4.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Figure 4: (Left) UMAP visualization of the molecules from ZINC250k and the generated samples with parp1 as the target protein. (Right) Distributional distances of the generated molecules measured by FCD and NSPDK MMD with respect to ZINC250k. </a> </div> <p>UMAP visualization (Figure 4, Left) shows that <strong>MOOD explores chemical space more broadly</strong> than competitors like REINVENT and FREED-QS, whose outputs cluster near the training data.<br/> MOOD’s samples visibly shift into new, unexplored regions, showing the role of the <strong>OOD term</strong>.</p> <p>Metrics such as <strong>FCD</strong> and <strong>NSPDK MMD</strong> (Figure 4, Right) further demonstrates this <strong>distributional divergence</strong>.</p> <h3 id="generated-molecules-and-visual-inspection">Generated Molecules and Visual Inspection</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig5-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/fig5.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Figure 5: Generated hit molecules with parp1 as the target protein and the corresponding ZINC250k molecules of the highest similarity. </a> </div> <p>Figures 5 compares the actual molecules produced by each method, and this shows that the baseline models tend to reproduce redundant motifs or slight variations of training data. On the other hand, <strong>MOOD’s molecules</strong> reveal <strong>low similarity</strong> to ZINC250k yet <strong>high binding affinity</strong>, demonstrating the <strong>novelty</strong> and <strong>utility</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig6-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/fig6.png" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Figure 6: Novel hit molecules found by MOOD against parp1 and the top 0.01% ZINC250k molecules. </a> </div> <p>Furthermore, Figure 6 highlights MOOD’s hits that <strong>outperform the top 0.01%</strong> of ZINC250k molecules in docking score while remaining dissimilar, underscoring MOOD’s <strong>effectiveness in discovering novel chemical optima</strong>.</p> <h3 id="comparison-to-3d-generation-models">Comparison to 3D Generation Models</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/table5-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/table5.png" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Table 5: Novel hit ratio (%) and novel top 5% docking score (kcal/mol) results with 3D molecule generation baselines and GDSS with respect to the target protein glmu. </a> </div> <p>MOOD is also benchmarked against modern 3D molecule generation models, such as Luo et al.<d-cite key="luo20213d"></d-cite> and Pocket2Mol<d-cite key="peng2022pocket2mol"></d-cite>, which leverages spatial binding site data. The result in Table 5 shows that MOOD outperforms both, even though it <strong>does not use any 3D information</strong>, emphasizing its <strong>practicality</strong> and <strong>generalization strength</strong>.</p> <h2 id="ablation-studies">Ablation Studies</h2> <p>To isolate the contributions of each core component in <strong>MOOD</strong>, the authors perform a comprehensive ablation study. This analysis investigates how both the <strong>OOD control mechanism</strong> and the <strong>property-guided gradient</strong> contribute to the quality and novelty of generated molecules.</p> <h3 id="effects-of-ood-control-and-property-gradient">Effects of OOD Control and Property Gradient</h3> <p>To understand the effect of each module, the following variants are compared: (1) <strong>MOOD-w/o Property Predictor</strong>, which disables property conditioning, keeping OOD guidance only (2) <strong>MOOD-w/o OOD Control</strong>, which disables $\lambda$-based novelty control, using only the property gradient (3) <strong>GDSS</strong>: baseline diffusion model without OOD or property guidance (4) <strong>Full MOOD</strong>, which combines both OOD control and property optimization.</p> <p>As shown in <strong>Tables 1–3</strong>, both OOD control and property optimization are necessary for achieving the best chemical optima, and this can be explained as:</p> <ul> <li> <p><strong>MOOD &gt; MOOD-w/o Property Predictor</strong>: Conditioning on biochemical properties boosts optimization.</p> </li> <li> <p><strong>MOOD-w/o Property Predictor &gt; GDSS</strong>: Demonstrates the standalone power of OOD exploration.</p> </li> <li> <p><strong>MOOD &gt; MOOD-w/o OOD Control</strong>: Confirms that novelty control further enhances exploration.</p> </li> <li> <p><strong>MOOD-w/o OOD Control &gt; GDSS</strong>: Even without property gradients, OOD-controlled diffusion finds more promising areas of chemical space. </p> </li> </ul> <h3 id="training-on-low-property-subsets-can-mood-discover-what-it-never-saw">Training on Low-Property Subsets: Can MOOD Discover What It Never Saw?</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/AI810/assets/img/2025-04-28-paper-2/fig7-1400.webp"/> <img src="/AI810/assets/img/2025-04-28-paper-2/fig7.png" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <a href="https://arxiv.org/abs/2206.07632" target="_blank">Figure 7: Top 5% docking score distribution of the molecules with respect to the target protein parp1. </a> </div> <p>To test MOOD’s ability to generalize beyond biased training data, the following model variants are trained on a <strong>low-quality subset</strong> of the ZINC250k dataset (bottom 50% ranked by $P_{\text{obj}}$): (1) <strong>L-MOOD-w/o OOD Control</strong>, which is trained only with property guidance (2) <strong>L-MOOD</strong>, which is trained with both OOD control and property guidance. The <strong>top 5% docking scores (DS)</strong> is evaluated for generated molecules that satisfy <strong>QED &gt; 0.5</strong> and <strong>SA &lt; 5</strong>. As shown in Figure 7, both <strong>L-MOOD</strong> and <strong>L-MOOD-w/o OOD Control</strong> outperform their low-quality training data, proving the effectiveness of <strong>property-based diffusion</strong>. Only <strong>L-MOOD</strong> surpasses the <strong>full ZINC250k dataset</strong> in top-5% DS, despite never seeing high-quality molecules during training. This shows that <strong>OOD-controlled diffusion</strong> not only fosters <strong>novelty</strong> but can also find <strong>superior optima in property space</strong>, effectively <strong>extrapolating beyond the limits of training data</strong>.</p> <h2 id="personal-thoughts">Personal Thoughts</h2> <p><strong>MOOD</strong> addresses the core limitation of <strong>molecular graph generation</strong> with the ability to explore <strong>novel chemical spaces</strong> while optimizing for <strong>real-world drug-like properties</strong>.</p> <p>By integrating <strong>OOD-controlled reverse-time diffusion</strong> with <strong>property-guided gradient optimization</strong>, MOOD creates a <strong>principled and controllable</strong> pathway for discovering molecules that are both <strong>chemically novel</strong> and <strong>pharmacologically relevant</strong>.</p> <h3 id="key-takeaways">Key Takeaways</h3> <ul> <li> <p><strong>Controlled Novelty through OOD-guided Diffusion</strong>:<br/> MOOD introduces a framework to bias sampling toward <strong>low-likelihood regions</strong> using a tunable $\lambda$ parameter. It is especially impressive that it only adds slight modification to the equation and does not require any further training, while leading to effective <strong>chemical diversity</strong> and <strong>exploration</strong>.</p> </li> <li> <p><strong>Multi-objective Property Optimization</strong>:<br/> Rather than optimizing a single property, MOOD uses a <strong>composite objective</strong>: \(P_{\text{obj}}(G_t) = \text{DS'}(G_t) \times \text{QED}(G_t) \times \text{SA'}(G_t)\) These are modeled through a <strong>learnable property predictor</strong> $P_\phi$, which provides gradient signals to steer generation.</p> </li> <li> <p><strong>Complementarity of OOD and Property Guidance</strong>:<br/> Ablation studies show that both components, OOD control and property gradients, are <strong>individually beneficial</strong> but <strong>synergistic when combined</strong>.</p> </li> <li> <p><strong>Generalization Beyond Training Distribution</strong>:<br/> Even when trained on <strong>low-quality subsets</strong> of the data, MOOD discovers <strong>superior molecules</strong> not seen during training, highlighting its <strong>extrapolative power</strong>.</p> </li> </ul> <h3 id="further-research-directions">Further Research Directions</h3> <p>Based on the proposed method, I believe that below approaches could be a possible further research direction.</p> <ul> <li> <p><strong>Incorporation of 3D Structural Information</strong><br/> Currently MOOD is limited to 2D graphs, so it could be extended to integrate <strong>3D binding pocket data</strong>, similar to approaches like Pocket2Mol. This would be especially beneficial for <strong>structure-based drug design</strong>, where spatial complementarity plays a critical role.</p> </li> <li> <p><strong>End-to-End Differentiable Docking</strong><br/> While MOOD uses a learned property predictor \(P_\phi\) to approximate scores like docking affinity, it does not simulate actual protein–ligand interactions. One possible extension could done by replacing or augmenting \(P_\phi\) with <strong>differentiable or learned docking simulators</strong>, such as DiffDock, EquiBind, that directly model the binding between a molecule and a protein target. This would allow end-to-end optimization grounded in real physical interaction modeling, improving the relevance of generated candidates for biochemical evaluation.</p> </li> <li> <p><strong>Multi-agent or Population-based Exploration</strong><br/> Inspired by <strong>evolutionary strategies</strong>, MOOD could be extended to use <strong>multiple agents or a population of models</strong> exploring different regions of chemical space in parallel. Such an approach would increase diversity and robustness in molecule discovery, potentially leading to more novel and optimal candidates by escaping local optima through collaborative or competitive exploration dynamics.</p> </li> <li> <p><strong>Generalization to Other Modalities</strong><br/> With the score-based generative modeling, OOD control, and property-conditioned sampling, MOOD could potentially be adapted to other structured domains like <strong>protein design</strong>, <strong>material discovery</strong>, or <strong>neural architecture search</strong>, where goal-conditioned and diverse generation is essential.</p> </li> </ul> <h2 id="extending-mood-incorporating-3d-structural-information">Extending MOOD: Incorporating 3D Structural Information</h2> <p>In this section, I propose a potential extension of MOOD by integrating <strong>3D structural information</strong> of target proteins into the generative process. While MOOD currently operates only in the 2D topological space of molecules, <strong>ligand–protein interactions are fundamentally 3D</strong>, and incorporating spatial constraints from protein binding pockets can improve biological relevance.</p> <p>The 3D-aware MOOD framework could be designed as follows:</p> <ol> <li> <p><strong>Protein Pocket Encoder</strong><br/> A neural encoder processes the 3D structure of a protein binding site, represented as a voxel grid, surface mesh, or point cloud, to produce a latent descriptor \(p \in \mathbb{R}^d,\) capturing shape and electrochemical features relevant for binding.</p> </li> <li> <p><strong>Conditioned Property Predictor</strong><br/> The property prediction network \(P_\phi(G_t, \lambda)\) is augmented to incorporate protein context as \(P_\phi(G_t, \lambda, p),\) allowing MOOD to optimize molecules for both chemical properties and spatial compatibility with the target site.</p> </li> <li> <p><strong>Modified Reverse-Time SDE</strong><br/> The conditional diffusion process is updated to guide generation with both OOD control and protein-specific gradients:</p> \[dG_t = \left[ f_t(G_t) - (1 - \sqrt{\lambda}) g_t^2 \nabla_{G_t} \log p_t(G_t) - \alpha_t g_t^2 \nabla_{G_t} P_\phi(G_t, \lambda, p) \right] dt + g_t d\bar{\omega}.\] </li> </ol> <p>This protein-aware extension of MOOD offers several advantages:</p> <ul> <li><strong>Target-specific Design</strong>: Molecules are generated to be spatially compatible with specific protein pockets, improving hit rates in structure-based drug discovery.</li> <li><strong>End-to-End Learning</strong>: Gradients from the property predictor can be backpropagated through the binding site encoder, enabling joint optimization of ligands and binding-site representations.</li> <li><strong>Flexible Integration</strong>: This formulation can incorporate various protein representations, including pretrained geometric encoders or differentiable docking modules.</li> </ul> <p>Possible issues such as the <strong>limited availability of aligned protein–ligand datasets</strong> and the increased complexity of modeling in 3D can be mitigated using transfer learning from structural databases such as PDBbind, or by leveraging pretrained protein encoders like those used in AlphaFold2 or EquiBind.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[Score-based generative models have shown promise in molecule generation, but often struggle to create truly novel candidates beyond the training distribution. MOOD (Molecular Out-Of-distribution Diffusion) is a score-based diffusion framework that enables controllable exploration of out-of-distribution chemical space without incurring additional computational costs. By integrating a property prediction network into the reverse-time SDE, MOOD effectively guides the generation toward molecules with desired novel traits such as high binding affinity, drug-likeness, and synthesizability.]]></summary></entry></feed>